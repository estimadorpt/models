name: ğŸ” Regression Testing Suite

on:
  # Run on all pull requests
  pull_request:
    branches: [ main, develop ]
    paths-ignore:
      - '*.md'
      - 'docs/**'
      - '.gitignore'
      - 'LICENSE'

  # Run on pushes to main branch
  push:
    branches: [ main ]
    paths-ignore:
      - '*.md'
      - 'docs/**'
      - '.gitignore'
      - 'LICENSE'

  # Allow manual triggering
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: true
        default: 'full'
        type: choice
        options:
          - quick
          - full
          - performance
      update_baselines:
        description: 'Update golden master baselines after successful run'
        required: false
        default: false
        type: boolean

env:
  # Ensure consistent behavior across runs
  PYTHONHASHSEED: 0
  PYTHONUTF8: 1
  
  # Performance settings
  OMP_NUM_THREADS: 2
  NUMBA_NUM_THREADS: 2

jobs:
  # Fast validation job - runs first to catch obvious issues
  quick-validation:
    name: ğŸš€ Quick Validation
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    outputs:
      should-continue: ${{ steps.changes.outputs.should-continue }}
      test-level: ${{ steps.setup.outputs.test-level }}
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 2  # Need history for change detection
    
    - name: ğŸ” Detect changes
      id: changes
      run: |
        # Check if this is a documentation-only change
        if git diff --name-only HEAD~1 HEAD | grep -E '\.(py|toml|yml|yaml|json)$'; then
          echo "should-continue=true" >> $GITHUB_OUTPUT
          echo "Code changes detected - proceeding with tests"
        else
          echo "should-continue=false" >> $GITHUB_OUTPUT  
          echo "No code changes detected - skipping regression tests"
        fi
    
    - name: âš™ï¸ Setup test parameters
      id: setup
      run: |
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "test-level=${{ github.event.inputs.test_level }}" >> $GITHUB_OUTPUT
        elif [ "${{ github.event_name }}" = "pull_request" ]; then
          echo "test-level=quick" >> $GITHUB_OUTPUT
        else
          echo "test-level=full" >> $GITHUB_OUTPUT
        fi
    
    - name: ğŸ Setup Python environment
      if: steps.changes.outputs.should-continue == 'true'
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: ğŸ“¦ Install pixi
      if: steps.changes.outputs.should-continue == 'true'
      uses: prefix-dev/setup-pixi@v0.4.1
      with:
        pixi-version: latest
    
    - name: ğŸ”§ Install dependencies
      if: steps.changes.outputs.should-continue == 'true'
      run: |
        pixi install
    
    - name: ğŸ§ª Run syntax validation
      if: steps.changes.outputs.should-continue == 'true'
      run: |
        echo "ğŸ” Validating Python syntax..."
        find src -name "*.py" -exec pixi run python -m py_compile {} +
        find tests -name "*.py" -exec pixi run python -m py_compile {} +
        find scripts -name "*.py" -exec pixi run python -m py_compile {} + 2>/dev/null || echo "No scripts directory found"
        
        echo "ğŸ” Validating import structure..."
        pixi run python -c "
        import sys
        sys.path.insert(0, 'src')
        try:
            from data.dataset import ElectionDataset
            from models.dynamic_gp_election_model import DynamicGPElectionModel
            print('âœ… Core imports successful')
        except ImportError as e:
            print(f'âŒ Import failed: {e}')
            sys.exit(1)
        "

  # Main regression testing matrix - runs different types in parallel  
  regression-testing:
    name: ğŸ”¬ Data & Interface Testing (${{ matrix.test-type }})
    runs-on: ubuntu-latest
    needs: quick-validation
    if: needs.quick-validation.outputs.should-continue == 'true'
    timeout-minutes: 15  # Fast validation only
    
    strategy:
      matrix:
        test-type: [data-validation, interface-testing]
        include:
          - test-type: data-validation
            timeout: 10
          - test-type: interface-testing
            timeout: 10
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
      with:
        lfs: true  # Ensure LFS files are downloaded
    
    - name: ğŸ Setup Python environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: ğŸ“¦ Install pixi
      uses: prefix-dev/setup-pixi@v0.4.1
      with:
        pixi-version: latest
    
    - name: ğŸ”§ Install dependencies and setup
      run: |
        pixi install
        
        # Create necessary directories
        mkdir -p outputs test_results
        
        # Set up git for potential commits (golden master updates)
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
    
    - name: ï¿½ Check project structure
      id: check-structure
      run: |
        echo "ğŸ” Validating project structure and data availability..."
        
        # Check critical data files
        echo "ğŸ“Š Checking data files..."
        if [ -d "data" ]; then
          echo "âœ… Data directory found"
          ls -la data/ | head -10
        else
          echo "âŒ Data directory missing"
          exit 1
        fi
        
        # Check source code structure  
        echo "ğŸ“ Checking source structure..."
        if [ -d "src" ]; then
          echo "âœ… Source directory found"
          find src -name "*.py" | head -10
        else
          echo "âŒ Source directory missing"
          exit 1
        fi
        
        # Check configuration files
        echo "âš™ï¸ Checking configuration..."
        if [ -f "pixi.toml" ]; then
          echo "âœ… Pixi configuration found"
        else
          echo "âŒ Pixi configuration missing"
          exit 1
        fi
    
    - name: ğŸ“Š Validate Data Integrity
      if: matrix.test-type == 'data-validation'
      timeout-minutes: ${{ matrix.timeout }}
      run: |
        echo "ğŸ“Š Validating data integrity and preprocessing..."
        
        # Check data files can be loaded
        pixi run python -c "
        import pandas as pd
        import sys
        import os
        
        print('ğŸ” Testing data loading...')
        
        # Test critical data files
        data_files = [
            'data/legislativas_2024.parquet',
            'data/municipal_coalitions_2025.parquet', 
            'data/gdp.csv',
            'data/geographic_mappings.csv'
        ]
        
        for file in data_files:
            if os.path.exists(file):
                try:
                    if file.endswith('.parquet'):
                        df = pd.read_parquet(file)
                    else:
                        df = pd.read_csv(file)
                    print(f'âœ… {file}: {len(df)} rows, {len(df.columns)} columns')
                    
                    # Basic sanity checks
                    if len(df) == 0:
                        print(f'âŒ {file}: Empty dataframe')
                        sys.exit(1)
                    if df.isnull().all().any():
                        print(f'âš ï¸ {file}: Contains completely null columns')
                        
                except Exception as e:
                    print(f'âŒ {file}: Failed to load - {e}')
                    sys.exit(1)
            else:
                print(f'âš ï¸ {file}: File not found (may be optional)')
        
        print('âœ… Data validation completed successfully')
        "
        
        # Test coalition data structure
        echo "ğŸ›ï¸ Validating coalition structure..."
        pixi run python -c "
        import json
        
        # Check coalition mappings if they exist
        try:
            with open('data/municipal_coalitions.json', 'r') as f:
                coalitions = json.load(f)
            print(f'âœ… Coalition mappings loaded: {len(coalitions)} entries')
            
            # Basic structure validation
            for key, value in list(coalitions.items())[:5]:  # Check first 5
                if not isinstance(value, dict):
                    print(f'âŒ Invalid coalition structure for {key}')
                    exit(1)
                    
        except FileNotFoundError:
            print('âš ï¸ Coalition mappings not found (may be generated dynamically)')
        except Exception as e:
            print(f'âŒ Coalition validation failed: {e}')
            exit(1)
        "
    
    - name: ğŸ§ª Test Model Interfaces  
      if: matrix.test-type == 'interface-testing'
      timeout-minutes: ${{ matrix.timeout }}
      run: |
        echo "ğŸ§ª Testing model interfaces and basic functionality..."
        
        # Test model instantiation and basic interface
        pixi run python -c "
        import sys
        sys.path.insert(0, 'src')
        
        print('ğŸ”§ Testing model instantiation...')
        try:
            from data.dataset import ElectionDataset
            from models.dynamic_gp_election_model import DynamicGPElectionModel
            print('âœ… Model imports successful')
            
            # Test dataset loading (minimal)
            print('ğŸ“Š Testing dataset loading...')
            dataset = ElectionDataset()
            print('âœ… Dataset initialized')
            
            # Test model instantiation  
            print('ğŸ—ï¸ Testing model instantiation...')
            model = DynamicGPElectionModel()
            print(f'âœ… Model instantiated: {type(model).__name__}')
            
            # Test basic configuration
            print('âš™ï¸ Testing model configuration...')
            config = model.get_config() if hasattr(model, 'get_config') else {}
            print('âœ… Model configuration accessible')
            
        except ImportError as e:
            print(f'âŒ Import failed: {e}')
            sys.exit(1)
        except Exception as e:
            print(f'âŒ Interface test failed: {e}')
            sys.exit(1)
        "
        
        # Test coalition handling (without training)
        echo "ğŸ›ï¸ Testing coalition handling logic..."
        pixi run python -c "
        import sys
        sys.path.insert(0, 'src')
        
        print('ğŸ” Testing coalition validation...')
        try:
            # Test basic coalition structure validation
            test_coalitions = {
                'Aveiro': {'AD': ['PSD', 'CDS'], 'PS': ['PS']},
                'Lisboa': {'AD': ['PSD', 'CDS'], 'BE': ['BE']}
            }
            
            # Basic validation - check structure is dict of dicts of lists
            for district, coalitions in test_coalitions.items():
                if not isinstance(coalitions, dict):
                    print(f'âŒ {district}: Invalid coalition structure')
                    sys.exit(1)
                for coalition, parties in coalitions.items():
                    if not isinstance(parties, list):
                        print(f'âŒ {district}.{coalition}: Parties must be list')
                        sys.exit(1)
                print(f'âœ… {district} coalition structure valid')
                
        except Exception as e:
            print(f'âŒ Coalition test failed: {e}')
            sys.exit(1)
        "
    
    - name: âœ… Validation Summary
      if: always()
      run: |
        echo "ğŸ“‹ Lightweight Validation Summary"
        echo "=================================="
        echo "âœ… Data integrity validation: Tests data loading and structure"
        echo "âœ… Interface testing: Validates model instantiation and basic APIs"
        echo "âœ… Coalition validation: Checks coalition data structure consistency"
        echo "âœ… Configuration validation: Verifies project setup and dependencies"
        echo ""
        echo "ğŸš€ This approach focuses on fast, practical validation that:"
        echo "   - Catches data preparation issues"
        echo "   - Validates code interfaces and imports"
        echo "   - Ensures configuration consistency"
        echo "   - Runs in <15 minutes instead of hours"
        echo ""
        echo "ğŸ’¡ Model training and statistical validation should be done:"
        echo "   - Locally during development"
        echo "   - On dedicated ML infrastructure"
        echo "   - As scheduled jobs with proper resources"
    
    - name: ğŸ“Š Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ matrix.test-type }}-${{ github.run_number }}
        path: |
          test_results/
          outputs/latest/
        retention-days: 30
    
    - name: ğŸ“‹ Display test summary
      if: always()
      run: |
        echo "ğŸ“‹ Test Summary"
        echo "=============="
        echo "Test Type: ${{ matrix.test-type }}"
        echo "Test Level: ${{ needs.quick-validation.outputs.test-level }}"
        echo "Branch: ${{ github.ref_name }}"
        echo "Commit: ${{ github.sha }}"
        
        if [ -f "test_results/regression_report.json" ]; then
          echo ""
          echo "ğŸ“Š Regression Detection Results:"
          pixi run python -c "
          import json
          with open('test_results/regression_report.json') as f:
              report = json.load(f)
          summary = report.get('summary', {})
          print(f'Health Status: {summary.get(\"health_status\", \"Unknown\")}')
          print(f'Success Rate: {summary.get(\"success_rate\", 0):.1%}')
          print(f'Tests: {summary.get(\"passed_tests\", 0)}/{summary.get(\"total_tests\", 0)}')
          print(f'Failures: {summary.get(\"total_failures\", 0)}')
          print(f'Warnings: {summary.get(\"total_warnings\", 0)}')
          "
        fi

  # Performance benchmarking job (runs on schedule or manual trigger)
  performance-benchmarking:
    name: âš¡ Performance Benchmarking
    runs-on: ubuntu-latest
    needs: quick-validation
    if: |
      needs.quick-validation.outputs.should-continue == 'true' && 
      (github.event_name == 'schedule' || 
       github.event.inputs.test_level == 'performance' ||
       needs.quick-validation.outputs.test-level == 'performance')
    timeout-minutes: 90
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
      with:
        lfs: true
    
    - name: ğŸ Setup Python environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: ğŸ“¦ Install pixi
      uses: prefix-dev/setup-pixi@v0.4.1
      with:
        pixi-version: latest
    
    - name: ğŸ”§ Install dependencies
      run: |
        pixi install
        mkdir -p performance_results
    
    - name: âš¡ Run performance benchmarks
      run: |
        echo "âš¡ Running performance benchmarks..."
        
        # Multiple runs for statistical significance
        for i in {1..3}; do
          echo "ğŸ”„ Performance run $i/3..."
          
          START_TIME=$(date +%s)
          
          pixi run python -m src.main \
            --mode train \
            --model-type dynamic_gp \
            --election-date 2024-03-10 \
            --output-dir performance_results/run_$i \
            --draws 200 \
            --tune 200 \
            --seed $((42 + i))
          
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          echo "Run $i completed in ${DURATION}s"
          echo "run_$i: ${DURATION}" >> performance_results/timing.txt
        done
        
        # Calculate average performance
        pixi run python -c "
        import numpy as np
        with open('performance_results/timing.txt') as f:
            times = [float(line.split(': ')[1]) for line in f]
        
        print(f'Performance Summary:')
        print(f'Average time: {np.mean(times):.1f}s')
        print(f'Std deviation: {np.std(times):.1f}s')
        print(f'Min time: {np.min(times):.1f}s')
        print(f'Max time: {np.max(times):.1f}s')
        "
    
    - name: ğŸ“Š Upload performance results
      uses: actions/upload-artifact@v4
      with:
        name: performance-results-${{ github.run_number }}
        path: performance_results/
        retention-days: 90

  # Golden master update job (only on successful main branch pushes)
  update-golden-masters:
    name: ğŸ”„ Update Golden Masters
    runs-on: ubuntu-latest
    needs: [regression-testing]
    if: |
      github.ref == 'refs/heads/main' && 
      github.event_name == 'push' &&
      github.event.inputs.update_baselines == 'true'
    timeout-minutes: 90
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        lfs: true
    
    - name: ğŸ Setup Python environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: ğŸ“¦ Install pixi
      uses: prefix-dev/setup-pixi@v0.4.1
      with:
        pixi-version: latest
    
    - name: ğŸ”§ Install dependencies
      run: pixi install
    
    - name: ğŸ”„ Generate fresh golden masters
      run: |
        echo "ğŸ”„ Generating updated golden master baselines..."
        
        # Backup existing baselines
        if [ -d "test_baselines" ]; then
          mv test_baselines test_baselines_backup_$(date +%Y%m%d_%H%M%S)
        fi
        
        # Generate new baselines
        pixi run python scripts/generate_golden_masters.py
        
        # Verify new baselines
        if [ ! -d "test_baselines" ]; then
          echo "âŒ Failed to generate new baselines"
          exit 1
        fi
        
        echo "âœ… New golden masters generated successfully"
    
    - name: ğŸ“¤ Commit updated baselines
      run: |
        git add test_baselines/
        
        if git diff --staged --quiet; then
          echo "No changes to golden masters"
        else
          git commit -m "ğŸ”„ Update golden master baselines

          - Generated from commit ${{ github.sha }}
          - All regression tests passed
          - Automated update via GitHub Actions"
          
          git push
          echo "âœ… Golden masters updated and committed"
        fi

  # Final status check
  regression-status:
    name: ğŸ“‹ Regression Testing Status
    runs-on: ubuntu-latest
    needs: [quick-validation, regression-testing]
    if: always()
    
    steps:
    - name: ğŸ“‹ Report final status
      run: |
        echo "ğŸ“‹ Final Regression Testing Status"
        echo "================================="
        
        if [ "${{ needs.quick-validation.outputs.should-continue }}" = "false" ]; then
          echo "â­ï¸  Skipped - No code changes detected"
          exit 0
        fi
        
        if [ "${{ needs.regression-testing.result }}" = "success" ]; then
          echo "âœ… All regression tests passed!"
          echo "âœ… System integrity maintained"
        elif [ "${{ needs.regression-testing.result }}" = "failure" ]; then
          echo "âŒ Regression tests failed!"
          echo "âŒ Please review the test results and fix any regressions"
          exit 1
        else
          echo "âš ï¸  Regression tests were cancelled or skipped"
          echo "âš ï¸  Please check the workflow logs"
          exit 1
        fi