name: 🔍 Regression Testing Suite

on:
  # Run on all pull requests
  pull_request:
    branches: [ main, develop ]
    paths-ignore:
      - '*.md'
      - 'docs/**'
      - '.gitignore'
      - 'LICENSE'

  # Run on pushes to main branch
  push:
    branches: [ main ]
    paths-ignore:
      - '*.md'
      - 'docs/**'
      - '.gitignore'
      - 'LICENSE'

  # Allow manual triggering
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: true
        default: 'full'
        type: choice
        options:
          - quick
          - full
          - performance
      update_baselines:
        description: 'Update golden master baselines after successful run'
        required: false
        default: false
        type: boolean

env:
  # Ensure consistent behavior across runs
  PYTHONHASHSEED: 0
  PYTHONUTF8: 1
  
  # Performance settings
  OMP_NUM_THREADS: 2
  NUMBA_NUM_THREADS: 2

jobs:
  # Fast validation job - runs first to catch obvious issues
  quick-validation:
    name: 🚀 Quick Validation
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    outputs:
      should-continue: ${{ steps.changes.outputs.should-continue }}
      test-level: ${{ steps.setup.outputs.test-level }}
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 2  # Need history for change detection
    
    - name: 🔍 Detect changes
      id: changes
      run: |
        # Check if this is a documentation-only change
        if git diff --name-only HEAD~1 HEAD | grep -E '\.(py|toml|yml|yaml|json)$'; then
          echo "should-continue=true" >> $GITHUB_OUTPUT
          echo "Code changes detected - proceeding with tests"
        else
          echo "should-continue=false" >> $GITHUB_OUTPUT  
          echo "No code changes detected - skipping regression tests"
        fi
    
    - name: ⚙️ Setup test parameters
      id: setup
      run: |
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "test-level=${{ github.event.inputs.test_level }}" >> $GITHUB_OUTPUT
        elif [ "${{ github.event_name }}" = "pull_request" ]; then
          echo "test-level=quick" >> $GITHUB_OUTPUT
        else
          echo "test-level=full" >> $GITHUB_OUTPUT
        fi
    
    - name: 🐍 Setup Python environment
      if: steps.changes.outputs.should-continue == 'true'
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: 📦 Install pixi
      if: steps.changes.outputs.should-continue == 'true'
      uses: prefix-dev/setup-pixi@v0.4.1
      with:
        pixi-version: latest
    
    - name: 🔧 Install dependencies
      if: steps.changes.outputs.should-continue == 'true'
      run: |
        pixi install
    
    - name: 🧪 Run syntax validation
      if: steps.changes.outputs.should-continue == 'true'
      run: |
        echo "🔍 Validating Python syntax..."
        find src -name "*.py" -exec pixi run python -m py_compile {} +
        find tests -name "*.py" -exec pixi run python -m py_compile {} +
        find scripts -name "*.py" -exec pixi run python -m py_compile {} + 2>/dev/null || echo "No scripts directory found"
        
        echo "🔍 Validating import structure..."
        pixi run python -c "
        import sys
        sys.path.insert(0, 'src')
        try:
            from data.dataset import ElectionDataset
            from models.dynamic_gp_election_model import DynamicGPElectionModel
            print('✅ Core imports successful')
        except ImportError as e:
            print(f'❌ Import failed: {e}')
            sys.exit(1)
        "

  # Main regression testing matrix - runs different types in parallel  
  regression-testing:
    name: 🔬 Data & Interface Testing (${{ matrix.test-type }})
    runs-on: ubuntu-latest
    needs: quick-validation
    if: needs.quick-validation.outputs.should-continue == 'true'
    timeout-minutes: 15  # Fast validation only
    
    strategy:
      matrix:
        test-type: [data-validation, interface-testing]
        include:
          - test-type: data-validation
            timeout: 10
          - test-type: interface-testing
            timeout: 10
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
      with:
        lfs: true  # Ensure LFS files are downloaded
    
    - name: 🐍 Setup Python environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: 📦 Install pixi
      uses: prefix-dev/setup-pixi@v0.4.1
      with:
        pixi-version: latest
    
    - name: 🔧 Install dependencies and setup
      run: |
        pixi install
        
        # Create necessary directories
        mkdir -p outputs test_results
        
        # Set up git for potential commits (golden master updates)
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
    
    - name: � Check project structure
      id: check-structure
      run: |
        echo "🔍 Validating project structure and data availability..."
        
        # Check critical data files
        echo "📊 Checking data files..."
        if [ -d "data" ]; then
          echo "✅ Data directory found"
          ls -la data/ | head -10
        else
          echo "❌ Data directory missing"
          exit 1
        fi
        
        # Check source code structure  
        echo "📁 Checking source structure..."
        if [ -d "src" ]; then
          echo "✅ Source directory found"
          find src -name "*.py" | head -10
        else
          echo "❌ Source directory missing"
          exit 1
        fi
        
        # Check configuration files
        echo "⚙️ Checking configuration..."
        if [ -f "pixi.toml" ]; then
          echo "✅ Pixi configuration found"
        else
          echo "❌ Pixi configuration missing"
          exit 1
        fi
    
    - name: 📊 Validate Data Integrity
      if: matrix.test-type == 'data-validation'
      timeout-minutes: ${{ matrix.timeout }}
      run: |
        echo "📊 Validating data integrity and preprocessing..."
        
        # Check data files can be loaded
        pixi run python -c "
        import pandas as pd
        import sys
        import os
        
        print('🔍 Testing data loading...')
        
        # Test critical data files
        data_files = [
            'data/legislativas_2024.parquet',
            'data/municipal_coalitions_2025.parquet', 
            'data/gdp.csv',
            'data/geographic_mappings.csv'
        ]
        
        for file in data_files:
            if os.path.exists(file):
                try:
                    if file.endswith('.parquet'):
                        df = pd.read_parquet(file)
                    else:
                        df = pd.read_csv(file)
                    print(f'✅ {file}: {len(df)} rows, {len(df.columns)} columns')
                    
                    # Basic sanity checks
                    if len(df) == 0:
                        print(f'❌ {file}: Empty dataframe')
                        sys.exit(1)
                    if df.isnull().all().any():
                        print(f'⚠️ {file}: Contains completely null columns')
                        
                except Exception as e:
                    print(f'❌ {file}: Failed to load - {e}')
                    sys.exit(1)
            else:
                print(f'⚠️ {file}: File not found (may be optional)')
        
        print('✅ Data validation completed successfully')
        "
        
        # Test coalition data structure
        echo "🏛️ Validating coalition structure..."
        pixi run python -c "
        import json
        
        # Check coalition mappings if they exist
        try:
            with open('data/municipal_coalitions.json', 'r') as f:
                coalitions = json.load(f)
            print(f'✅ Coalition mappings loaded: {len(coalitions)} entries')
            
            # Basic structure validation
            for key, value in list(coalitions.items())[:5]:  # Check first 5
                if not isinstance(value, dict):
                    print(f'❌ Invalid coalition structure for {key}')
                    exit(1)
                    
        except FileNotFoundError:
            print('⚠️ Coalition mappings not found (may be generated dynamically)')
        except Exception as e:
            print(f'❌ Coalition validation failed: {e}')
            exit(1)
        "
    
    - name: 🧪 Test Model Interfaces  
      if: matrix.test-type == 'interface-testing'
      timeout-minutes: ${{ matrix.timeout }}
      run: |
        echo "🧪 Testing model interfaces and basic functionality..."
        
        # Test model instantiation and basic interface
        pixi run python -c "
        import sys
        sys.path.insert(0, 'src')
        
        print('🔧 Testing model instantiation...')
        try:
            from data.dataset import ElectionDataset
            from models.dynamic_gp_election_model import DynamicGPElectionModel
            print('✅ Model imports successful')
            
            # Test dataset loading (minimal)
            print('📊 Testing dataset loading...')
            dataset = ElectionDataset(
                election_date='2026-01-01',  # Future test date
                baseline_timescales=[365],   # 1 year baseline
                election_timescales=[30, 15] # Campaign periods
            )
            print('✅ Dataset initialized')
            
            # Test model instantiation  
            print('🏗️ Testing model instantiation...')
            model = DynamicGPElectionModel(dataset=dataset)
            print(f'✅ Model instantiated: {type(model).__name__}')
            
            # Test basic configuration
            print('⚙️ Testing model configuration...')
            config = model.get_config() if hasattr(model, 'get_config') else {}
            print('✅ Model configuration accessible')
            
        except ImportError as e:
            print(f'❌ Import failed: {e}')
            sys.exit(1)
        except Exception as e:
            print(f'❌ Interface test failed: {e}')
            sys.exit(1)
        "
        
        # Test coalition handling (without training)
        echo "🏛️ Testing coalition handling logic..."
        pixi run python -c "
        import sys
        sys.path.insert(0, 'src')
        
        print('🔍 Testing coalition validation...')
        try:
            # Test basic coalition structure validation
            test_coalitions = {
                'Aveiro': {'AD': ['PSD', 'CDS'], 'PS': ['PS']},
                'Lisboa': {'AD': ['PSD', 'CDS'], 'BE': ['BE']}
            }
            
            # Basic validation - check structure is dict of dicts of lists
            for district, coalitions in test_coalitions.items():
                if not isinstance(coalitions, dict):
                    print(f'❌ {district}: Invalid coalition structure')
                    sys.exit(1)
                for coalition, parties in coalitions.items():
                    if not isinstance(parties, list):
                        print(f'❌ {district}.{coalition}: Parties must be list')
                        sys.exit(1)
                print(f'✅ {district} coalition structure valid')
                
        except Exception as e:
            print(f'❌ Coalition test failed: {e}')
            sys.exit(1)
        "
    
    - name: ✅ Validation Summary
      if: always()
      run: |
        echo "📋 Lightweight Validation Summary"
        echo "=================================="
        echo "✅ Data integrity validation: Tests data loading and structure"
        echo "✅ Interface testing: Validates model instantiation and basic APIs"
        echo "✅ Coalition validation: Checks coalition data structure consistency"
        echo "✅ Configuration validation: Verifies project setup and dependencies"
        echo ""
        echo "🚀 This approach focuses on fast, practical validation that:"
        echo "   - Catches data preparation issues"
        echo "   - Validates code interfaces and imports"
        echo "   - Ensures configuration consistency"
        echo "   - Runs in <15 minutes instead of hours"
        echo ""
        echo "💡 Model training and statistical validation should be done:"
        echo "   - Locally during development"
        echo "   - On dedicated ML infrastructure"
        echo "   - As scheduled jobs with proper resources"
    
    - name: 📊 Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ matrix.test-type }}-${{ github.run_number }}
        path: |
          test_results/
          outputs/latest/
        retention-days: 30
    
    - name: 📋 Display test summary
      if: always()
      run: |
        echo "📋 Test Summary"
        echo "=============="
        echo "Test Type: ${{ matrix.test-type }}"
        echo "Test Level: ${{ needs.quick-validation.outputs.test-level }}"
        echo "Branch: ${{ github.ref_name }}"
        echo "Commit: ${{ github.sha }}"
        
        if [ -f "test_results/regression_report.json" ]; then
          echo ""
          echo "📊 Regression Detection Results:"
          pixi run python -c "
          import json
          with open('test_results/regression_report.json') as f:
              report = json.load(f)
          summary = report.get('summary', {})
          print(f'Health Status: {summary.get(\"health_status\", \"Unknown\")}')
          print(f'Success Rate: {summary.get(\"success_rate\", 0):.1%}')
          print(f'Tests: {summary.get(\"passed_tests\", 0)}/{summary.get(\"total_tests\", 0)}')
          print(f'Failures: {summary.get(\"total_failures\", 0)}')
          print(f'Warnings: {summary.get(\"total_warnings\", 0)}')
          "
        fi

  # Performance benchmarking job (runs on schedule or manual trigger)
  performance-benchmarking:
    name: ⚡ Performance Benchmarking
    runs-on: ubuntu-latest
    needs: quick-validation
    if: |
      needs.quick-validation.outputs.should-continue == 'true' && 
      (github.event_name == 'schedule' || 
       github.event.inputs.test_level == 'performance' ||
       needs.quick-validation.outputs.test-level == 'performance')
    timeout-minutes: 15  # Lightweight validation should complete quickly
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
      with:
        lfs: true
    
    - name: 🐍 Setup Python environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: 📦 Install pixi
      uses: prefix-dev/setup-pixi@v0.4.1
      with:
        pixi-version: latest
    
    - name: 🔧 Install dependencies
      run: |
        pixi install
        mkdir -p performance_results
    
    - name: ⚡ Run lightweight performance validation
      run: |
        echo "⚡ Running lightweight performance validation..."
        
        # Test instantiation and basic operation speed (no training)
        START_TIME=$(date +%s.%N)
        
        pixi run python -c "
        import sys
        import time
        sys.path.insert(0, 'src')
        
        # Test dataset loading performance
        dataset_start = time.time()
        from data.dataset import ElectionDataset
        dataset = ElectionDataset(
            election_date='2024-03-10',
            baseline_timescales=[365],
            election_timescales=[30, 15]
        )
        dataset_time = time.time() - dataset_start
        
        # Test model instantiation performance
        model_start = time.time()
        from models.dynamic_gp_election_model import DynamicGPElectionModel
        model = DynamicGPElectionModel(dataset=dataset)
        model_time = time.time() - model_start
        
        print(f'Dataset loading: {dataset_time:.2f}s')
        print(f'Model instantiation: {dataset_time:.2f}s')
        print(f'Total interface time: {dataset_time + model_time:.2f}s')
        
        # Performance validation
        if dataset_time > 10:
            print('⚠️ Dataset loading is slower than expected (>10s)')
        if model_time > 5:
            print('⚠️ Model instantiation is slower than expected (>5s)')
            
        print('✅ Performance validation completed')
        "
    
    - name: 📊 Upload validation results
      uses: actions/upload-artifact@v4
      with:
        name: lightweight-validation-results-${{ github.run_number }}
        path: test_results/
        retention-days: 30

  # Golden master update job (only on successful main branch pushes)
  update-golden-masters:
    name: 🔄 Update Golden Masters
    runs-on: ubuntu-latest
    needs: [regression-testing]
    if: |
      github.ref == 'refs/heads/main' && 
      github.event_name == 'push' &&
      github.event.inputs.update_baselines == 'true'
    timeout-minutes: 15  # Lightweight validation should complete quickly
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        lfs: true
    
    - name: 🐍 Setup Python environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: 📦 Install pixi
      uses: prefix-dev/setup-pixi@v0.4.1
      with:
        pixi-version: latest
    
    - name: 🔧 Install dependencies
      run: pixi install
    
    - name: 🔄 Validate golden masters compatibility
      run: |
        echo "🔄 Validating baseline compatibility without full training..."
        
        # Test that we can load and parse existing baselines if they exist
        if [ -d "test_baselines" ]; then
          echo "📁 Found existing baselines, validating structure..."
          pixi run python -c "
import json
from pathlib import Path

# Validate baseline directory structure and metadata
test_baselines = Path('test_baselines')
if not test_baselines.exists():
    print('No baselines directory found')
    exit(0)

# Check for metadata file
metadata_file = test_baselines / 'golden_masters_metadata.json'
if metadata_file.exists():
    with open(metadata_file) as f:
        metadata = json.load(f)
    print(f'✅ Valid metadata found (created: {metadata.get(\"created_at\", \"unknown\")})')
    print(f'   Model type: {metadata.get(\"model_type\", \"unknown\")}')
    print(f'   Election date: {metadata.get(\"election_date\", \"unknown\")}')
else:
    print('⚠️  No metadata file found, baselines may be outdated')

# Check directory structure
dirs_to_check = ['train', 'predict', 'viz']
for dir_name in dirs_to_check:
    dir_path = test_baselines / dir_name
    if dir_path.exists():
        print(f'✅ {dir_name}/ directory exists')
    else:
        print(f'⚠️  {dir_name}/ directory missing')

print('✅ Baseline structure validation completed')
"
        else
          echo "📝 No existing baselines found - this is OK for new branches"
        fi
        
        # Test that baseline generation interfaces work (without actually training)
        echo "🧪 Testing baseline generation interface compatibility..."
        pixi run python -c "
# Test that we can import and validate the baseline generation modules
import sys
import os
sys.path.insert(0, 'src')

try:
    from main import main
    print('✅ Main module imports successfully')
    
    # Test command line argument parsing for train mode
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', choices=['train', 'predict', 'viz'])
    parser.add_argument('--model-type', default='dynamic_gp')
    parser.add_argument('--election-date')
    parser.add_argument('--output-dir')
    parser.add_argument('--draws', type=int, default=100)
    parser.add_argument('--tune', type=int, default=100)
    parser.add_argument('--seed', type=int, default=42)
    
    # Test that argument parsing works
    test_args = parser.parse_args(['--mode', 'train', '--model-type', 'dynamic_gp'])
    print('✅ Command line interface parsing works')
    
    print('✅ All baseline generation interfaces validated')
    
except ImportError as e:
    print(f'❌ Import error: {e}')
    exit(1)
except Exception as e:
    print(f'❌ Validation error: {e}') 
    exit(1)
"
        
        echo "✅ Golden masters compatibility validation completed"

  # Final status check
  regression-status:
    name: 📋 Regression Testing Status
    runs-on: ubuntu-latest
    needs: [quick-validation, regression-testing]
    if: always()
    
    steps:
    - name: 📋 Report final status
      run: |
        echo "📋 Final Regression Testing Status"
        echo "================================="
        
        if [ "${{ needs.quick-validation.outputs.should-continue }}" = "false" ]; then
          echo "⏭️  Skipped - No code changes detected"
          exit 0
        fi
        
        if [ "${{ needs.regression-testing.result }}" = "success" ]; then
          echo "✅ All regression tests passed!"
          echo "✅ System integrity maintained"
        elif [ "${{ needs.regression-testing.result }}" = "failure" ]; then
          echo "❌ Regression tests failed!"
          echo "❌ Please review the test results and fix any regressions"
          exit 1
        else
          echo "⚠️  Regression tests were cancelled or skipped"
          echo "⚠️  Please check the workflow logs"
          exit 1
        fi