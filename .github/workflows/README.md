# GitHub Actions CI/CD Workflows

This directory contains comprehensive CI/CD workflows for the election modeling pipeline, implementing automated regression testing and quality assurance.

## ğŸ”„ Workflow Overview

### 1. ğŸ” Regression Testing (`regression-testing.yml`)

**Primary workflow for change validation**

- **Triggers**: Pull requests, pushes to main, manual dispatch
- **Purpose**: Ensure no regressions are introduced by changes
- **Duration**: 10-60 minutes depending on test level

**Test Levels**:
- `quick`: Coalition + deterministic tests (10 min)
- `full`: Complete pipeline integration (45 min) 
- `performance`: Performance benchmarking (90 min)

**Key Features**:
- âœ… Automatic change detection (skips docs-only changes)
- ğŸ”¬ Bit-for-bit comparison against golden masters
- ğŸ“Š Statistical validation of model outputs
- âš¡ Performance regression detection
- ğŸ¤ Coalition handling validation

### 2. ğŸŒ™ Nightly Comprehensive Testing (`nightly-comprehensive-testing.yml`)

**Comprehensive system health monitoring**

- **Triggers**: Daily at 2 AM UTC, manual dispatch
- **Purpose**: Deep validation and early issue detection
- **Duration**: Up to 3 hours

**Features**:
- ğŸ§ª Full integration test suite with coverage
- ğŸ” Comprehensive regression detection
- âš¡ Multi-configuration performance benchmarking
- ğŸ”¬ Data quality validation
- ğŸš¨ Automatic issue creation on failures
- âœ… Auto-closing of resolved issues

### 3. âœ… Pull Request Validation (`pr-validation.yml`)

**Fast feedback for pull requests**

- **Triggers**: PR events (opened, synchronized, etc.)
- **Purpose**: Quick validation and developer feedback
- **Duration**: 10-45 minutes

**Features**:
- ğŸš€ Smart change analysis
- ğŸ¯ Targeted testing based on changes
- ğŸ“ Automatic PR comments with results
- ğŸ” Regression testing for significant changes
- ğŸ§ª Test validation for test-only changes

## ğŸ“Š Testing Infrastructure

### Golden Master Baselines

Located in `test_baselines/`, these provide reference outputs for regression detection:

```
test_baselines/
â”œâ”€â”€ golden_masters_metadata.json    # Configuration and timestamps
â””â”€â”€ train/                          # Training outputs
    â”œâ”€â”€ model_config.json          # Model configuration
    â”œâ”€â”€ fit_metrics.json           # Fit quality metrics
    â”œâ”€â”€ predictions/               # Prediction outputs
    â”‚   â”œâ”€â”€ vote_share_summary_election_day.csv
    â”‚   â””â”€â”€ total_seat_summary_direct_election_day.csv
    â””â”€â”€ visualizations/            # Generated plots
```

### Regression Detection Tools

The `scripts/regression_detection_tools.py` provides comprehensive comparison capabilities:

- **BitForBitComparator**: Exact reproducibility validation
- **StatisticalComparator**: Model output validation with MCMC tolerance
- **PerformanceRegessionDetector**: Execution time monitoring
- **CoalitionValidationDetector**: TARGET-ELECTION-DRIVEN approach validation

### Test Suites

- `tests/integration/test_current_system_behavior.py`: Full pipeline testing
- `tests/regression/`: Existing validation framework
- `scripts/test_regression_tools.py`: Tool validation

## ğŸ”§ Configuration

### Environment Requirements

- **Python**: 3.11
- **Package Manager**: pixi
- **Key Dependencies**: PyMC, pandas, numpy, pytest

### Environment Variables

```bash
PYTHONHASHSEED=0      # Deterministic behavior
PYTHONUTF8=1          # Consistent encoding
OMP_NUM_THREADS=2     # Control parallelism
NUMBA_NUM_THREADS=2   # Control compilation
```

### Test Thresholds

- **Vote Share Difference**: Â±2 percentage points
- **Seat Difference**: Â±2 seats
- **Fit Metric Regression**: 30% tolerance
- **Performance Regression**: 2x slowdown threshold

## ğŸš€ Usage Guide

### For Developers

1. **Making Changes**:
   - Create feature branch from `main`
   - Make your changes
   - Push to trigger PR validation
   - Address any regression test failures

2. **PR Process**:
   - PR validation runs automatically
   - Check PR comments for regression results
   - Green checkmarks = ready for review
   - Red X = fix regressions before merge

3. **Interpreting Results**:
   - âœ… All green: No regressions detected
   - âš ï¸ Warnings: Minor changes within tolerance
   - âŒ Failures: Regressions detected, requires fixes

### For Maintainers

1. **Golden Master Updates**:
   ```bash
   # Generate new baselines after approved changes
   pixi run python scripts/generate_golden_masters.py
   
   # Commit and push
   git add test_baselines/
   git commit -m "ğŸ”„ Update golden master baselines"
   git push
   ```

2. **Manual Workflow Triggers**:
   - Go to Actions tab in GitHub
   - Select workflow to run
   - Click "Run workflow"
   - Choose parameters (test level, etc.)

3. **Monitoring Health**:
   - Check nightly validation results
   - Review performance trends
   - Address failure issues promptly

## ğŸ” Troubleshooting

### Common Issues

1. **Golden Masters Missing**:
   ```bash
   # Generate fresh baselines
   pixi run python scripts/generate_golden_masters.py
   ```

2. **Import Errors**:
   ```bash
   # Check environment setup
   pixi install
   pixi run python -c "import src.main"
   ```

3. **Performance Regressions**:
   - Check for inefficient code changes
   - Review MCMC parameter changes
   - Compare with historical benchmarks

4. **Coalition Handling Issues**:
   - Verify AD=PSD+CDS coalition structure
   - Check TARGET-ELECTION-DRIVEN approach
   - Review coalition manager changes

### Debugging Workflows

1. **Download Artifacts**:
   - Go to failed workflow run
   - Download test result artifacts
   - Review detailed logs and reports

2. **Local Reproduction**:
   ```bash
   # Run same tests locally
   pixi run python -m pytest tests/integration/ -v
   
   # Run regression detection
   pixi run python scripts/regression_detection_tools.py \
     --golden-masters test_baselines \
     --current-output path/to/output \
     --quick
   ```

3. **Check Logs**:
   - Review workflow step logs
   - Look for error messages and stack traces
   - Check artifact contents

## ğŸ“ˆ Monitoring and Metrics

### Key Metrics Tracked

- **Test Success Rate**: Percentage of passing tests
- **Performance Trends**: Execution time over time  
- **Data Quality**: Coalition structure, data consistency
- **Coverage**: Code coverage from integration tests

### Health Indicators

- **âœ… Healthy**: All tests pass, no regressions
- **âš ï¸ Warning**: Minor issues, some warnings
- **âŒ Critical**: Test failures, significant regressions

### Alerts

- Nightly validation failures create GitHub issues
- Performance regressions trigger warnings
- Data quality issues generate alerts
- Golden master generation failures notify maintainers

## ğŸ”„ Maintenance

### Regular Tasks

1. **Weekly**: Review nightly validation trends
2. **Monthly**: Update golden masters if needed
3. **Quarterly**: Review and optimize workflows
4. **Annually**: Update CI/CD documentation

### Golden Master Management

- **Update Trigger**: Approved model improvements
- **Validation**: All regression tests must pass
- **Backup**: Previous baselines archived automatically
- **Documentation**: Changes documented in commit messages

### Performance Optimization

- Monitor workflow execution times
- Optimize test parallelization
- Cache dependencies when possible
- Use appropriate test levels for different triggers

---

## ğŸ“š Related Documentation

- [Regression Testing Guide](../docs/regression-testing.md)
- [Golden Master Management](../docs/golden-master-management.md)
- [Coalition Handling Documentation](../docs/coalition_handling_documentation.md)
- [Performance Benchmarking](../docs/performance-benchmarking.md)

## ğŸ¤ Contributing

When modifying workflows:

1. Test changes in feature branches
2. Update documentation
3. Ensure backward compatibility
4. Review impact on CI/CD performance
5. Get approval from maintainers

For questions or issues with CI/CD workflows, please create an issue with the `ci-cd` label.