import argparse
import json
import os
import pandas as pd
import re
from datetime import datetime

# Expected column names (ArviZ defaults)
MEAN_COL_NAME = 'mean'
SD_COL_NAME = 'sd' # ArviZ default for standard deviation
# HDI column names will be generated by get_arviz_hdi_column_names

# Assumed HDI probability for constructing column names
ASSUMED_HDI_PROB = 0.94

def get_arviz_hdi_column_names(hdi_prob=ASSUMED_HDI_PROB):
    """Generates ArviZ default HDI column names based on probability."""
    # Calculate percentage points for the labels (e.g., 3.0 for 3%, 97.0 for 97%)
    perc_low_label = (1 - hdi_prob) / 2 * 100
    perc_high_label = (1 + hdi_prob) / 2 * 100 # Corresponds to the upper boundary, e.g., 97 for hdi_prob=0.94

    # Format these percentage points using :.2g, similar to ArviZ's internal formatting
    # This handles cases like 3.0 -> "3" and 2.5 -> "2.5"
    low_str = f"{perc_low_label:.2g}"
    high_str = f"{perc_high_label:.2g}"
    
    return {
        "hdi_low": f'hdi_{low_str}%',  # e.g., hdi_3% or hdi_2.5%
        "hdi_high": f'hdi_{high_str}%', # e.g., hdi_97% or hdi_97.5%
                                        # For 0.94 hdi_prob, (1+0.94)/2*100 = 97.0, f"{97.0:.2g}" is "97". This is correct.
    }

def get_run_dirs(run1_path, run2_path, latest_vs_previous, base_outputs_dir="outputs/"):
    """
    Determines the directories for the two runs to be compared.
    Returns (path_to_run_A, path_to_run_B)
    """
    if latest_vs_previous:
        latest_symlink = os.path.join(base_outputs_dir, "latest")
        if not os.path.islink(latest_symlink):
            raise FileNotFoundError(f"Symlink '{latest_symlink}' not found. Cannot determine latest run.")
        
        run_a_dir = os.path.realpath(latest_symlink)
        
        run_pattern = r"^(?:static|dynamic_gp)_run_(\d{8}_\d{6})$"
        potential_runs = []
        for item in os.listdir(base_outputs_dir):
            full_path = os.path.join(base_outputs_dir, item)
            if os.path.isdir(full_path) and item != "latest": # Exclude the 'latest' symlink itself
                match = re.match(run_pattern, item)
                if match:
                    try:
                        timestamp_str = match.group(1)
                        dt_obj = datetime.strptime(timestamp_str, "%Y%m%d_%H%M%S")
                        potential_runs.append({"path": full_path, "datetime": dt_obj, "name": item})
                    except ValueError:
                        print(f"Warning: Could not parse timestamp from directory name: {item}")
                        continue
        
        if not potential_runs:
            raise FileNotFoundError(f"No parsable run directories found in '{base_outputs_dir}'.")

        # Sort runs by datetime, newest first
        potential_runs.sort(key=lambda x: x["datetime"], reverse=True)
        
        run_a_resolved_path = os.path.normpath(run_a_dir)
        run_a_index = -1
        for i, run_info in enumerate(potential_runs):
            if os.path.normpath(run_info["path"]) == run_a_resolved_path:
                run_a_index = i
                break
        
        if run_a_index == -1:
             raise FileNotFoundError(f"Latest run '{run_a_dir}' (from symlink) not found among parsed run directories in '{base_outputs_dir}'. Ensure it matches the pattern.")

        if run_a_index + 1 >= len(potential_runs):
            # This means the 'latest' run is the only run, or the oldest if sorting failed to place it first
            print(f"Warning: Latest run '{run_a_dir}' is the only run found or the oldest. Cannot find a previous run for comparison.")
            return os.path.normpath(run_a_dir), None # Return None for run_b_dir
            
        run_b_dir = potential_runs[run_a_index + 1]["path"]
        print(f"Run A (latest): {run_a_dir}")
        print(f"Run B (previous): {run_b_dir}")
        return os.path.normpath(run_a_dir), os.path.normpath(run_b_dir)
    else:
        if not run1_path or not run2_path:
            raise ValueError("If not using --latest-vs-previous, both --run1 and --run2 must be specified.")
        if not os.path.isdir(run1_path):
            raise FileNotFoundError(f"Run directory not found: {run1_path}")
        if not os.path.isdir(run2_path):
            raise FileNotFoundError(f"Run directory not found: {run2_path}")
        print(f"Run A: {run1_path}")
        print(f"Run B: {run2_path}")
        return os.path.normpath(run1_path), os.path.normpath(run2_path)

def load_prediction_csv(run_dir, prediction_mode, data_type, hdi_prob=ASSUMED_HDI_PROB):
    """
    Loads vote share or seat summary CSV from a run directory.
    Assumes ArviZ default column names like 'mean', 'sd', 'hdi_3%', 'hdi_97%'.
    data_type can be 'vote_share' or 'seat'.
    """
    arviz_hdi_cols = get_arviz_hdi_column_names(hdi_prob)
    base_file_name = None
    alternative_file_name = None # For seats from direct district method

    if data_type == "vote_share":
        base_file_name = f"vote_share_summary_{prediction_mode}.csv"
    elif data_type == "seat":
        base_file_name = f"seat_summary_{prediction_mode}.csv"
        alternative_file_name = f"seat_summary_direct_{prediction_mode}.csv" # Used by DynamicGPModel
    else:
        raise ValueError(f"Invalid data_type: {data_type}. Must be 'vote_share' or 'seat'.")

    file_path = os.path.join(run_dir, "predictions", base_file_name)
    actual_file_used = base_file_name

    if not os.path.exists(file_path) and alternative_file_name:
        file_path_alt = os.path.join(run_dir, "predictions", alternative_file_name)
        if os.path.exists(file_path_alt):
            file_path = file_path_alt
            actual_file_used = alternative_file_name
            print(f"Primary file {base_file_name} not found, using alternative {alternative_file_name}")
        else:
            raise FileNotFoundError(f"Prediction file not found: Neither {base_file_name} nor {alternative_file_name} exist in {os.path.join(run_dir, 'predictions')}")
    elif not os.path.exists(file_path):
        raise FileNotFoundError(f"Prediction file not found: {file_path}")

    print(f"Loading data from: {actual_file_used}")
    try:
        df = pd.read_csv(file_path, index_col=0) # Parties are in the first column (index)
    except Exception as e:
        raise IOError(f"Error reading CSV file {file_path}: {e}")

    # Define expected ArviZ columns and how they map to internal consistent names
    # We'll use 'mean_value', 'sd_value', 'hdi_low_value', 'hdi_high_value' internally
    # regardless of data_type (vote_share or seat) for simplicity in perform_comparison.
    expected_arviz_cols = {
        MEAN_COL_NAME: 'mean_value',         # 'mean' -> 'mean_value'
        SD_COL_NAME: 'sd_value',             # 'sd' -> 'sd_value'
        arviz_hdi_cols["hdi_low"]: 'hdi_low_value',
        arviz_hdi_cols["hdi_high"]: 'hdi_high_value'
    }
    
    missing_cols = [col for col in expected_arviz_cols.keys() if col not in df.columns]
    if missing_cols:
        raise ValueError(f"Missing expected ArviZ default columns in {file_path}: {missing_cols}. Available: {df.columns.tolist()}")

    # Select only the expected ArviZ columns and rename them to the internal consistent names
    df_processed = df[list(expected_arviz_cols.keys())].rename(columns=expected_arviz_cols)
    # df_processed.attrs['mean_col_name_original'] = mean_col_name # No longer needed
    return df_processed

def perform_comparison(run_a_dir, run_b_dir, prediction_mode, hdi_prob=ASSUMED_HDI_PROB):
    """
    Performs the core comparison logic between two runs.
    Returns a dictionary with the comparison results.
    """
    comparison_results = {
        "metadata": {
            "run_A_path": run_a_dir,
            "run_B_path": run_b_dir,
            "prediction_date_mode": prediction_mode,
            "hdi_prob_used_for_columns": hdi_prob,
            "comparison_timestamp": datetime.now().isoformat()
        },
        "vote_share_comparison": {},
        "seat_comparison": {}
    }

    for data_type, result_key in [
        ("vote_share", "vote_share_comparison"),
        ("seat", "seat_comparison")
    ]:
        try:
            print(f"Loading {data_type} data for Run A ({run_a_dir})...")
            data_a = load_prediction_csv(run_a_dir, prediction_mode, data_type, hdi_prob)
            data_b = None
            if run_b_dir: # Only load if run_b_dir is provided (not None)
                print(f"Loading {data_type} data for Run B ({run_b_dir})...")
                data_b = load_prediction_csv(run_b_dir, prediction_mode, data_type, hdi_prob)
            
            # The internal standardized names for mean is 'mean_value'
            current_mean_col_internal = 'mean_value'

            all_parties = sorted(list(set(data_a.index) | (set(data_b.index) if data_b is not None else set())))

            for party in all_parties:
                party_comparison = {}
                
                data_a_party = data_a.loc[party] if party in data_a.index else pd.Series(dtype='float64')
                party_comparison["run_A"] = data_a_party.to_dict()

                change_in_mean = None
                mean_a = data_a_party.get(current_mean_col_internal)

                if data_b is not None:
                    data_b_party = data_b.loc[party] if party in data_b.index else pd.Series(dtype='float64')
                    party_comparison["run_B"] = data_b_party.to_dict()
                    mean_b = data_b_party.get(current_mean_col_internal)
                    
                    if pd.notnull(mean_a) and pd.notnull(mean_b):
                        change_in_mean = mean_a - mean_b
                    elif pd.notnull(mean_a): # Only A has it
                        change_in_mean = mean_a
                    elif pd.notnull(mean_b): # Only B has it
                        change_in_mean = -mean_b
                else: # No run_B data
                    party_comparison["run_B"] = {} # Empty dict for run_B
                    if pd.notnull(mean_a):
                        change_in_mean = mean_a # Change is just the value from run_A
                    
                party_comparison["change"] = {current_mean_col_internal: change_in_mean}
                comparison_results[result_key][party] = party_comparison

        except (FileNotFoundError, ValueError, IOError) as e:
            print(f"Error processing {data_type} for comparison: {e}")
            comparison_results[result_key]['error'] = str(e)

    return comparison_results

def main():
    parser = argparse.ArgumentParser(description="Compare predictions between two model runs.")
    parser.add_argument("--run1", help="Path to the first model run directory (often the newer or 'A' run).")
    parser.add_argument("--run2", help="Path to the second model run directory (often the older or 'B' run).")
    parser.add_argument("--latest-vs-previous", action="store_true",
                        help="Automatically compare the 'latest' run with the one immediately preceding it. Ignores --run1 and --run2 if set.")
    parser.add_argument("--prediction-date-mode", default="election_day",
                        choices=['election_day', 'last_poll', 'today'],
                        help="Prediction date mode to compare (default: election_day).")
    parser.add_argument("--output-file", required=True,
                        help="Path to save the JSON output file for the comparison.")
    parser.add_argument("--hdi-prob", type=float, default=ASSUMED_HDI_PROB,
                        help=f"HDI probability used in the model runs (default: {ASSUMED_HDI_PROB}), to find correct column names.")

    args = parser.parse_args()

    try:
        run_a_dir, run_b_dir = get_run_dirs(args.run1, args.run2, args.latest_vs_previous)
        if run_b_dir is None and args.latest_vs_previous:
             print(f"Cannot perform latest-vs-previous comparison as no previous run was found for {run_a_dir}.")
             # Create a "comparison" that only contains data for run_A
             comparison_data = perform_comparison(run_a_dir, None, args.prediction_date_mode, args.hdi_prob)
             comparison_data["metadata"]["notes"] = "Comparison with previous run skipped: No previous run found."
        elif run_b_dir is None and not args.latest_vs_previous:
             print(f"Error: Run B directory was not determined, and not in latest-vs-previous mode.")
             return # Should not happen if arg validation in get_run_dirs is correct
        else:
            comparison_data = perform_comparison(run_a_dir, run_b_dir, args.prediction_date_mode, args.hdi_prob)

    except (FileNotFoundError, ValueError, IOError) as e:
        print(f"Error setting up or performing comparison: {e}")
        # Create a minimal JSON output indicating failure
        comparison_data = {
            "metadata": {
                "error": str(e),
                "run_A_path": args.run1 if not args.latest_vs_previous else "latest (resolution failed)",
                "run_B_path": args.run2 if not args.latest_vs_previous else "previous (resolution failed)",
                "prediction_date_mode": args.prediction_date_mode,
                "comparison_timestamp": datetime.now().isoformat()
            },
            "vote_share_comparison": {"error": str(e)},
            "seat_comparison": {"error": str(e)}
        }

    try:
        os.makedirs(os.path.dirname(args.output_file), exist_ok=True) # Ensure output dir exists
        with open(args.output_file, 'w') as f:
            # Handle NaN and other non-serializable types like numpy numbers gracefully
            def default_converter(o):
                if isinstance(o, (pd.Timestamp, datetime)):
                    return o.isoformat()
                if pd.isna(o):
                    return None
                if hasattr(o, 'item'): # Handles numpy numbers
                    return o.item()
                raise TypeError(f"Object of type {o.__class__.__name__} is not JSON serializable")

            json.dump(comparison_data, f, indent=4, default=default_converter)
        print(f"Comparison results saved to {args.output_file}")
    except IOError as e:
        print(f"Error writing output JSON to {args.output_file}: {e}")
    except Exception as e: # Catch any other serialization errors
        print(f"An unexpected error occurred during JSON serialization: {e}")

if __name__ == "__main__":
    main() 